"""
KBO ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ 2025 ì‹œì¦Œ SSG ëœë”ìŠ¤ íƒ€ì ê¸°ë¡ ìˆ˜ì§‘
playwrightë¥¼ ì´ìš©í•œ í¬ë¡¤ë§ ì½”ë“œ

ì‹¤í–‰ ë°©ë²•:
    pip install playwright pandas
    playwright install chromium
    python 01_crawl_kbo.py
"""

import asyncio
import pandas as pd
from playwright.async_api import async_playwright


BASE_URL = "https://www.koreabaseball.com/Record/Player/HitterBasic/Basic1.aspx"
YEAR = "2025"
TEAM = "SSG"


async def select_filters(page):
    """ì—°ë„, íŒ€ í•„í„° ì„ íƒ"""
    # ì—°ë„ ì„ íƒ
    await page.select_option('select:nth-of-type(1)', YEAR)
    await page.wait_for_timeout(800)

    # íŒ€ ì„ íƒ (SSG)
    await page.select_option('select:nth-of-type(3)', TEAM)
    await page.wait_for_timeout(1000)


async def parse_table(page) -> list[dict]:
    """í˜„ì¬ í˜ì´ì§€ì˜ íƒ€ì ê¸°ë¡ í…Œì´ë¸” íŒŒì‹±"""
    rows = await page.query_selector_all('table tbody tr')
    records = []

    for row in rows:
        cells = await row.query_selector_all('td')
        if len(cells) < 14:
            continue

        texts = [await c.inner_text() for c in cells]
        records.append({
            'ìˆœìœ„':   texts[0].strip(),
            'ì„ ìˆ˜ëª…': texts[1].strip(),
            'íŒ€':     texts[2].strip(),
            'AVG':    texts[3].strip(),
            'G':      texts[4].strip(),
            'PA':     texts[5].strip(),
            'AB':     texts[6].strip(),
            'R':      texts[7].strip(),
            'H':      texts[8].strip(),
            '2B':     texts[9].strip(),
            '3B':     texts[10].strip(),
            'HR':     texts[11].strip(),
            'TB':     texts[12].strip(),
            'RBI':    texts[13].strip(),
        })

    return records


async def crawl_page2(page) -> list[dict]:
    """2í˜ì´ì§€ ì´ë™ í›„ íŒŒì‹±"""
    next_btn = await page.query_selector('a[href*="btnNo2"]')
    if next_btn:
        await next_btn.click()
        await page.wait_for_timeout(1200)
        return await parse_table(page)
    return []


async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        print(f"[1/4] KBO ì‚¬ì´íŠ¸ ì ‘ì† ì¤‘...")
        await page.goto(BASE_URL, wait_until="domcontentloaded")
        await page.wait_for_timeout(1000)

        print(f"[2/4] í•„í„° ì„¤ì • (ì—°ë„: {YEAR}, íŒ€: {TEAM})...")
        await select_filters(page)

        print(f"[3/4] 1í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        records = await parse_table(page)

        print(f"[4/4] 2í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        records += await crawl_page2(page)

        await browser.close()

    # SSG ì„ ìˆ˜ë§Œ í•„í„°ë§
    df = pd.DataFrame(records)
    df = df[df['íŒ€'] == 'SSG'].reset_index(drop=True)

    # ìˆ«ìí˜• ë³€í™˜
    num_cols = ['AVG', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B', 'HR', 'TB', 'RBI']
    for col in num_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # ìµœì†Œ íƒ€ì„ í•„í„° (ê·œì • íƒ€ì„: ê²½ê¸°ìˆ˜ Ã— 3.1 â‰ˆ 144 Ã— 3.1 = 446)
    df_qualified = df[df['PA'] >= 200].copy()  # ë¶„ì„ í¬í•¨ ê¸°ì¤€ì€ 200íƒ€ì„ ì´ìƒ

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: SSG ì„ ìˆ˜ ì´ {len(df)}ëª… (200íƒ€ì„ ì´ìƒ: {len(df_qualified)}ëª…)")
    print(df_qualified[['ì„ ìˆ˜ëª…', 'G', 'PA', 'AB', 'AVG', 'HR', 'RBI', 'TB']].to_string(index=False))

    # CSV ì €ì¥
    df.to_csv('data/ssg_hitters_raw.csv', index=False, encoding='utf-8-sig')
    df_qualified.to_csv('data/ssg_hitters_qualified.csv', index=False, encoding='utf-8-sig')
    print("\nğŸ’¾ data/ í´ë”ì— CSV ì €ì¥ ì™„ë£Œ")


if __name__ == "__main__":
    import os
    os.makedirs("data", exist_ok=True)
    asyncio.run(main())
